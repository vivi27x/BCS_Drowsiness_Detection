<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>RELATED WORKS&colon;</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <p>We aim to develop of a drowsiness monitoring system to continuously watch the vigilance state of the driver and send alarm before the driver falls asleep. However, building a calibration-free drowsiness recognition system is still a challenging task. The difficulty lies in capturing common drowsiness-related patterns from a diversity of EEG signals with a low signal-to-noise rate.</p>
<h2 id="related-works">RELATED WORKS:</h2>
<ol>
<li><strong>Existing driver monitor systems :</strong></li>
</ol>
<ul>
<li>CCD camera with infrared LED detectors by Toyota.</li>
<li>Focus and Mondeo models judging lane tracking performance by Ford</li>
<li>biometric car seat prototype tracking physiological signals by Ford
and others.</li>
</ul>
<ol start="2">
<li><strong>EEG-based driver drowsiness recognition :</strong>
EEG measures voltage difference on the scalp resulted from ion currents caused by synaptic activities of thousands of pyramidal neurons happening on the surface layer of the brain underneath.  Existing EEG systems have 1 to 256 electrodes, and the placement of electrodes follows a formal standard called <strong>10â€“20 system</strong> or <strong>International 10â€“20 system.</strong>
Existing studies have revealed the relationship between drowsiness and the oscillation patterns of EEG signals.
It was summarized by Klimesch that drowsiness can in general cause an increase in power of the Theta and Alpha frequency bands. Moreover, increase in lower Alpha power occurs only when subjects are struggling not to sleep, while the Alpha power will decrease when subjects fall asleep.
For the task proposed a simple convolutional network consisting of two convolutional layers, a max-pooling layer, a flatten and a fully connected layer to classify single-channel EEG data for Advance Driver Assistance Systems (ADAS) of automotive, was proposed.</li>
</ol>
<h2 id="materials-and-methods">MATERIALS AND METHODS</h2>
<h3 id="data-preparation">DATA PREPARATION:</h3>
<ul>
<li>The dataset
was collected from 27 subjects (aged from 22 to 28)</li>
<li>The participants were required to respond immediately to the events by driving the car
back to central lane. The drowsiness level can be reflected by how fast the subjects respond to the events.</li>
<li>sampled at 500 Hz with 30 electrodes and processed with 1-50 Hz bandpass filters and artifact rejection</li>
<li>Each sample has a dimension of 30 (channels) Ã— 384 (sample points)</li>
<li><strong>local reaction time (RT)</strong>: the time taken by the subject to respond to the car drift event, <strong>global-RT</strong>: which is the average of RTs within a 90-second window before the car drift event</li>
<li>The <strong>baseline â€œalert-RTâ€</strong> for each
session was defined as the 5th percentile of the local RTs. Samples with both local-RT and global-RT shorter than 1.5 times alert-RT were labeled as alert state, while samples with both local-RT and global-RT longer than 2.5 times alert-RT were labeled as drowsy state</li>
<li>discarded sessions with less than 50 samples of either class.</li>
<li>performed these two additional selection procedures in order to loosely balance the samples for each class and each subject</li>
<li>finally got
an unbalanced dataset of 2952 samples from 11 different
subjects</li>
</ul>
<h3 id="network-design-">NETWORK DESIGN :</h3>
<ul>
<li>
<p>Suppose the EEG signals recorded from m electrodes are {ğ’™ğ’Š} ğ‘–=1,2â€¦ğ‘š. ğ‘1 new signals {ğ’”ğ’‹} ğ‘—=1,2â€¦ğ‘1 can be obtained from linear combination of the original m signals.
<img src="file:///d:\Drowsiness\_resources\952389f427bd5667de682fd7a3a16101.png" alt="952389f427bd5667de682fd7a3a16101.png">
The weights can be found using methods like Independent Component Analysis (ICA), Common Spatial Pattern (CSP), etc.</p>
</li>
<li>
<p>The proposed processing sequence of EEG signal is similar
to the concept of depthwise separable convolution. A <strong>depthwise separable convolution</strong>, or called <strong>â€œseparable convolutionâ€</strong>, consists of a depthwise convolution, which is performed on each channel independently, and a pointwise convolution (or called â€œ1x1 convolutionâ€), which projects the channels onto a new space.</p>
</li>
<li>
<p>The network consists of seven layers.
<img src="file:///d:\Drowsiness\_resources\b4727285dc6d43c6e01d2ba3fc29a95a.png" alt="b4727285dc6d43c6e01d2ba3fc29a95a.png">
The pointwise convolution and depthwise convolution are implemented in the first two layers, which are followed by a ReLU activation layer, a batch normalization layer, a global average pooling layer, a dense layer and a Softmax activation layer.</p>
</li>
</ul>
<p>In the first layer, we use ğ‘1 pointwise convolutional nodes to generate ğ‘1 new channels of signals with the same length. The outputs from the first layer:
<img src="file:///d:\Drowsiness\_resources\4488a4feea7d32f6c6c972a45a2a993f.png" alt="4488a4feea7d32f6c6c972a45a2a993f.png">
where i = 1, 2, 3, â€¦, ğ‘1 and <strong>ğ‘¥<sub>ğ‘,ğ‘—</sub></strong> is the j-th sampling point of the p-th channel of the input EEG sample. The superscripts of the outputs and network parameters indicate which network
layer they belong to.</p>
<p>In the second layer, depthwise convolutions are used to extract features from the ğ‘1 obtained signals using 2ğ‘1 depthwise convolutional nodes. The output:
<img src="file:///d:\Drowsiness\_resources\413753b7479eb7df8e00bf30c7755ebf.png" alt="413753b7479eb7df8e00bf30c7755ebf.png"></p>
<p>For the consequent layers:
<img src="file:///d:\Drowsiness\_resources\d5636e2c97f4cfac446648dd0410bede.png" alt="d5636e2c97f4cfac446648dd0410bede.png">
<img src="file:///d:\Drowsiness\_resources\cbc340064d257aa90a76a979a02b4fcd.png" alt="cbc340064d257aa90a76a979a02b4fcd.png">
<img src="file:///d:\Drowsiness\_resources\02ff8aa7f59f84b06fc74f0aad8638cb.png" alt="02ff8aa7f59f84b06fc74f0aad8638cb.png">
<img src="file:///d:\Drowsiness\_resources\37f63aa681cfc6c369521f5dabbef1b1.png" alt="37f63aa681cfc6c369521f5dabbef1b1.png"></p>
<h3 id="interpretation-technique">INTERPRETATION TECHNIQUE:</h3>
<p>The Class Activation Map (CAM) method is a powerful interpretation technique that can localize the discriminative regions of each input sample for a CNNmodel trained to solve a classification task. For each input sample a heatmap is generated from the activations after the last convolutional layer. CAM method was originally designed for deep CNN networks with only standard convolutional layers and cannot be used directly for our model.
Our objective is to find the heatmap ğ‘†<sub>(ğ‘š,ğ‘›)</sub><sup>ğ¶</sup>
for ğ‘‹(ğ‘š,ğ‘›) that can reveal important regions for the prediction by the network. We have,
<img src="file:///d:\Drowsiness\_resources\3c343cd4142f8837d16e250f998c2f21.png" alt="3c343cd4142f8837d16e250f998c2f21.png">
<strong>ğ‘€<sub>ğ‘–,ğ‘—</sub><sup>ğ‘</sup></strong> is the activation map of class c for the sample <strong>ğ‘‹<sub>(ğ‘š,ğ‘›)</sub></strong>.</p>
<p>The first (channel) dimension of <strong>ğ‘€<sub>ğ‘–,ğ‘—</sub><sup>ğ‘</sup></strong> misaligned with the first (channel) dimension of the input signal. We trace only a small portion of the positions in the activation map that contribute most to the class activation <strong>â„<sub>ğ‘</sub><sup>(6)</sup></strong>. Specifically, we rank the values of <strong>ğ‘€<sub>ğ‘–,ğ‘—</sub><sup>ğ‘</sup></strong> in a descending order.</p>
<p>The final heatmap for sample  <strong>ğ‘‹<sub>(ğ‘š,ğ‘›)</sub></strong> can be obtained by combining all the class discriminative points in the input sample with the Gaussian function.
<img src="file:///d:\Drowsiness\_resources\eec0e5ca0de3e01f5109d7e744bff106.png" alt="eec0e5ca0de3e01f5109d7e744bff106.png">
ğœ is a constant that decides radius of the influential area of each discriminative point in the input signal. The output is further normalized to (-1,1).</p>
<p>The discriminative locations are unchanged after the 3rd and 4th layers of the network. From previous relations, we have:
<img src="file:///d:\Drowsiness\_resources\b56e21a6fe85fb2294ab1c407eca678d.png" alt="b56e21a6fe85fb2294ab1c407eca678d.png">
<img src="file:///d:\Drowsiness\_resources\c4c4d430458e4c13ee544833b3be28cf.png" alt="c4c4d430458e4c13ee544833b3be28cf.png"></p>
<p>Therefore, the discriminative location <strong>(ğ‘–<sub>ğ‘˜</sub>,ğ‘—<sub>ğ‘˜</sub>)</strong> in <strong>ğ‘€<sub>ğ‘–,ğ‘—</sub><sup>ğ‘</sup></strong> can be traced back to the center <strong>(ğ‘<sub>ğ‘˜</sub>, ğ‘<sub>ğ‘˜</sub>)</strong> of the strongest contributing episode in the input signal, where:
<img src="file:///d:\Drowsiness\_resources\8ec94ca1dcd999c0bdcfdcff64d3bb7b.png" alt="8ec94ca1dcd999c0bdcfdcff64d3bb7b.png"></p>
<h3 id="methods-for-comparison">METHODS FOR COMPARISON:</h3>
<ol>
<li><strong>Deep learning methods:</strong>  The study compares deep learning models for EEG signal classification, including EEGNet-4,2, EEGNet-8,2, Sinc-ShallowNet, and Conv-ShallowNet. Key components include 2D and depthwise convolutions, sinc functions, and the evaluation of cross-subject driver drowsiness recognition.</li>
<li><strong>Conventional baseline methods:</strong> We implement five baseline methods for feature extraction and test them on eight different classifiers for comparison:</li>
</ol>
<ul>
<li><strong>Relative Power:</strong> It uses relative band powers as the first baseline method.</li>
<li><strong>Log Power :</strong> natural log of the band power instead of
relative power is calculated.</li>
<li><strong>Power Ratio:</strong>  four band power ratios (i) (Î¸+Î±)/Î² , (ii) Î±/Î², (iii) (Î¸+ Î±)/(Î±+ Î²) and (iv) Î¸/Î² were found to be good indicators of driver drowsiness</li>
<li><strong>Wavelet Entropy:</strong>  The wavelet entropy feature for each EEG channel is calculated by applying the <strong>Shannon function</strong> on the normalized wavelet coefficients.</li>
<li><strong>Four Entropies:</strong> We use four types of entropies, which are sample entropy, fuzzy entropy, approximate entropy and spectral entropy for driver fatigue recognition.</li>
<li><strong>Classifiers:</strong> Different classifiers have been implemented,
which include Decision Tree (DT), Random Forest (RF), k-nearest neighbors (KNeighbors), Gaussian Naive Bayes (GNB), Logistic Regression (LR), LDA, Quadratic Discriminant Analysis (QDA), and SVM</li>
</ul>
<ol start="3">
<li><strong>Variations of the model for comparison:</strong></li>
</ol>
<ul>
<li>we replace the pointwise and the depthwise convolutional layers of
the network with a standard convolutional layer containing 32
kernels with length of 64. We name this model as â€œ1DConvâ€.</li>
<li>In the second and the third variations, we remove the
depthwise convolutional layer and the pointwise convolutional
layer and name as â€œNoDepthwiseâ€ and â€œNoPointwiseâ€, repectively.</li>
<li>Finally, the batch normalization layer is removed. We name the model as
â€œNoBatchNormâ€.</li>
</ul>
<h3 id="implementation-details">IMPLEMENTATION DETAILS:</h3>
<p>The comparison study was conducted on an Alienware Desktop with a 64-bit Windows 10 OS, Intel i7-6700 CPU, and NVIDIA GeForce GTX 1080 GPU, using Python 3.6.6. Deep learning models, including the proposed model and Sinc-ShallowNet variations, were implemented with PyTorch, while EEGNet models ran on TensorFlow's Keras API. Initial tests revealed accuracy drops due to default batch normalization, which was mitigated by disabling mean and variance estimation. The models were trained with the Adam optimizer, batch size of 50, and default parameters (Î·=0.001, Î²1=0.9, Î²2=0.999). Conventional methods employed Welch's method for feature extraction and sklearn library classifiers with default settings.</p>
<h2 id="evaluation-on-the-proposed-method">EVALUATION ON THE PROPOSED METHOD</h2>
<h3 id="model-comparison-results">MODEL COMPARISON RESULTS:</h3>
<p>we conduct leave-one-subject-out cross validation to compare the classifiers. Specifically, the EEG data from one subject are used for testing, while data from all the other subjects are used for training the classifiers. The process is iterated until every subject serves once as the test subject.</p>
<ol>
<li><strong>Mean accuracy comparison on the balanced dataset:</strong>
We trained each deep learning model from 1 to 50 epochs. We randomized the network parameters for each iteration and repeated the process for 10 times. In this way, 10 (times) x 11 (subjects) = 110 folds were created for each epoch.
<img src="file:///d:\Drowsiness\_resources\41b6a31f1ca6ded5135427dadd45ed33.png" alt="41b6a31f1ca6ded5135427dadd45ed33.png"></li>
<li><strong>Individual comparison results on the unbalanced dataset:</strong>
Here we compare the best baseline methods with the proposed method on unbalanced data for each individual. Specifically, we conduct leave-one-subject-out cross validation, where the unbalanced EEG data from one subject is used for testing, while the balanced data from all the other
subjects are used for training in order to obtain the unbiased classifiers. We use precision and recall to evaluate the classification on the unbalanced data.
<img src="file:///d:\Drowsiness\_resources\755126b91a77de50712a090b2a86d320.png" alt="755126b91a77de50712a090b2a86d320.png"></li>
</ol>
<h3 id="interpretation-on-the-learned-characteristics-from-eeg-signals">Interpretation on the learned characteristics from EEG signals:</h3>
<p>The study aims to derive insights into model validation by analyzing patterns learned by the model to distinguish between alert and drowsy EEG signals.</p>
<ul>
<li>Representative samples correctly classified with high confidence show that drowsy signals often contain high portions of Theta and Alpha waves.</li>
<li>Specifically, Theta band bursts and Alpha spindles are strong indicators of drowsiness, with central EEG channels playing a crucial role.</li>
<li>Alert signals, on the other hand, often contain more artifacts, such as Beta waves from peripheral channels and Delta waves from eye movements, which are mistakenly identified as alertness indicators.</li>
<li>We find that sensor noise contained in the EEG signals is one of the major reasons that lead to the low classification accuracy</li>
</ul>

            
            
        </body>
        </html>